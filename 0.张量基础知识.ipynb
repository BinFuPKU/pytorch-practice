{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch 是Facebook推出的一款深度学习框架，通过动态编译极大减轻了开发者的工作量，方便快速开发实现模型。\n",
    "# 本部分为个人pytorch练习，把握其内在机制和实现细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fubin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代损失=0.15837982296943665\n",
      "第100次迭代损失=0.5138764381408691\n",
      "第200次迭代损失=0.2319364994764328\n",
      "第300次迭代损失=0.09167849272489548\n",
      "第400次迭代损失=0.27909207344055176\n",
      "第500次迭代损失=0.4107387959957123\n",
      "第600次迭代损失=0.19654621183872223\n",
      "第700次迭代损失=0.377009779214859\n",
      "第800次迭代损失=0.31133395433425903\n",
      "第900次迭代损失=0.1997448354959488\n"
     ]
    }
   ],
   "source": [
    "# 构建第一个神经网络来学习一个固定的函数，神经网络具有强大的拟合能力。\n",
    "# [128,32] + ReLU + [32,8] +  ReLU + [8, 1] + Sigmoid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 目标函数\n",
    "def func(x):\n",
    "    return sum([x[:,i]/float(2*i+2) for i in range(x.shape[1])])\n",
    "\n",
    "# print(func(np.random.rand(3,4)))\n",
    "\n",
    "# 模型\n",
    "import torch\n",
    "# print(dir(torch.nn))\n",
    "from torch.nn import Linear, ReLU, Sequential, Sigmoid\n",
    "NET = Sequential(\n",
    "#     Linear(128,32),\n",
    "#     ReLU(),\n",
    "#     Linear(32,8),\n",
    "#     ReLU(),\n",
    "    Linear(8,1),\n",
    "    Sigmoid())\n",
    "\n",
    "# input = torch.randn((10,128))\n",
    "# print(input)\n",
    "# print(NET(input).squeeze().detach()) # .squeeze(), torch.squeeze()\n",
    "# print(torch.squeeze(NET(input)).detach()) # .squeeze(), torch.squeeze()\n",
    "\n",
    "# 优化\n",
    "from torch.optim import Adam, SGD\n",
    "optimizer = SGD(NET.parameters(), lr=0.001,momentum=0.1) # 冲量系数\n",
    "# optimizer = Adam(NET.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.L1Loss()\n",
    "criterion = torch.nn.SmoothL1Loss()\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad() # 梯度置0\n",
    "    # 输入数据\n",
    "    input = torch.randn((10,8))\n",
    "    # 生成标签\n",
    "    label = func(input.detach())\n",
    "    # 模型输出\n",
    "    output = NET(input).squeeze()\n",
    "    # 平方损失\n",
    "#     loss = torch.sum((label-output)**2)\n",
    "    loss = criterion(output, torch.tensor(label))\n",
    "    # 优化\n",
    "    loss.backward() # 梯度反向计算\n",
    "    optimizer.step() # 梯度更新\n",
    "    if step%100==0:\n",
    "        print('第{}次迭代损失={}'.format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor([[0.4321, 0.6524]])\n",
      ".size(): torch.Size([1, 2])\n",
      ".dim(): 2\n",
      ".numel(): 2\n",
      ".dtype: torch.float32\n",
      "tensor([[0.7703, 0.2616]], dtype=torch.float64)\n",
      "tensor([[0.],\n",
      "        [0.]])\n",
      "tensor([[0., 0.]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[1., 1.]])\n",
      "tensor([[78],\n",
      "        [78]])\n",
      "tensor([[87., 87.]])\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "tensor([0., 0.])\n",
      "tensor([[2.3495e+35, 2.0005e+00]])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000, 1.0000])\n",
      "tensor([1, 2, 3])\n",
      "tensor([ 0.0000,  3.3333,  6.6667, 10.0000])\n",
      "tensor([1.0000e+00, 3.5938e+00, 1.2915e+01, 4.6416e+01, 1.6681e+02, 5.9948e+02,\n",
      "        2.1544e+03, 7.7426e+03, 2.7826e+04, 1.0000e+05])\n",
      "tensor([0.0478, 0.4494, 0.0777, 0.8772])\n",
      "tensor([[0.0969, 0.7815]])\n",
      "tensor([ 0.3298, -0.1942])\n",
      "tensor([[0.2169, 0.4921]])\n",
      "tensor([[-0.3568, -0.2769]])\n",
      "normal: tensor([ 0.9602, 11.1494])\n",
      "tensor([[1, 0, 2],\n",
      "        [0, 2, 0]])\n",
      "tensor([[15., 16.]])\n",
      "tensor([[1., 1.]])\n",
      "tensor([[1, 0]])\n",
      "tensor([2, 4, 3, 0, 1])\n",
      ".reshape(*): torch.Size([2, 1])\n",
      "torch.reshape: torch.Size([2, 1])\n",
      ".reshape(-1): torch.Size([2])\n",
      "squeeze: torch.Size([2])\n",
      "unsqueeze: torch.Size([1, 2, 1])\n",
      "transpose: torch.Size([2, 1])\n",
      "transpose: torch.Size([2, 1])\n",
      "permute: torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fubin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "/Users/fubin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n"
     ]
    }
   ],
   "source": [
    "# 张量及其属性和用法\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "data = torch.rand((1,2))\n",
    "print('data:', data)\n",
    "# 属性\n",
    "print('.size():', data.size())\n",
    "print('.dim():', data.dim())\n",
    "print('.numel():', data.numel()) # number of elements\n",
    "print('.dtype:', data.dtype)\n",
    "# 创建张量\n",
    "# 传入数据\n",
    "print(torch.tensor(np.random.rand(1,2)))\n",
    "# 0\n",
    "print(torch.zeros((2,1)))\n",
    "print(torch.zeros_like(data))\n",
    "# 1\n",
    "print(torch.ones((2,1)))\n",
    "print(torch.ones_like(data))\n",
    "# 指定值\n",
    "print(torch.full((2,1), 78))\n",
    "print(torch.full_like(data, 87))\n",
    "# 对角\n",
    "print(torch.eye(2))\n",
    "# 空(未初始化)\n",
    "print(torch.empty(2))\n",
    "print(torch.empty_like(data))\n",
    "# 等差数列\n",
    "print(torch.range(1,4, step=1)) # [start, end]\n",
    "print(torch.range(0,1, step=0.2)) # [start, end]\n",
    "print(torch.arange(1,4, step=1)) # [start, end)\n",
    "print(torch.linspace(0,10, steps=4)) # start, end, steps:  delta=(end-start)/steps\n",
    "# 等比数列\n",
    "print(torch.logspace(0,5, steps=10)) # start, end, steps:  delta=(10^end-10^start)/steps\n",
    "# 随机生成： 标准均匀分布\n",
    "print(torch.rand(4))\n",
    "print(torch.rand_like(data))\n",
    "# 随机生成： 标准正态分布\n",
    "print(torch.randn(2))\n",
    "print(torch.randn_like(data))\n",
    "print(torch.normal(data))\n",
    "# 随机生成： 指定正态分布（对应位置）\n",
    "mean, std = torch.tensor([0.,4.]), torch.tensor([1.,10.])\n",
    "print('normal:', torch.normal(mean, std))\n",
    "# 随机生成： 离散均匀分布\n",
    "print(torch.randint(0,3, (2,3))) # low, high, size\n",
    "print(torch.randint_like(data, 10, 20)) # data, low, high\n",
    "# 两点伯努利分布(概率为0值的概率)\n",
    "print(torch.bernoulli(data))\n",
    "# 多个两点分布\n",
    "# 权重，n_samples取样次数\n",
    "print(torch.multinomial(data, 2))\n",
    "# 二维张量则返回二维张量，即每一行都取了n_samples次\n",
    "\n",
    "# 随机排列\n",
    "print(torch.randperm(5))\n",
    "\n",
    "# 变形\n",
    "print('.reshape(*):', data.reshape(2,1).shape)\n",
    "print('torch.reshape:', torch.reshape(data, (2,1)).shape)\n",
    "print('.reshape(-1):', data.reshape(-1).shape) # 打平\n",
    "# 压缩维度长度为1\n",
    "print('squeeze:', data.squeeze(dim=0).shape)\n",
    "# 扩张一个维度，为1\n",
    "print('unsqueeze:', data.unsqueeze(dim=-1).shape)\n",
    "# 转置两个维度\n",
    "print('transpose:', data.transpose(0,1).shape)\n",
    "print('transpose:', data.t().shape) # 直接转置\n",
    "# 指定各维度的顺序\n",
    "print('permute:', data.permute(1,0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data= tensor([[0.4666, 0.0856, 0.0436],\n",
      "        [0.4339, 0.0489, 0.8822]])\n",
      "index_select: tensor([[0.4666, 0.0436],\n",
      "        [0.4339, 0.8822]])\n",
      ": tensor([[0.0856],\n",
      "        [0.0489]])\n",
      "masked_select: tensor([0.0856, 0.4339, 0.8822])\n",
      "take: tensor([0.0856, 0.0436, 0.0489])\n",
      "cat: tensor([[0.4666, 0.0856, 0.0436],\n",
      "        [0.4339, 0.0489, 0.8822],\n",
      "        [0.4666, 0.0856, 0.0436],\n",
      "        [0.4339, 0.0489, 0.8822]])\n",
      "stack: torch.Size([2, 3, 2])\n",
      "repeat: torch.Size([4, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fubin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1356.)\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# 张量的高级用法\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "data = torch.rand((2,3))\n",
    "print('data=', data)\n",
    "\n",
    "# 选择部分元素: 某一维度，选择哪几个元素\n",
    "print('index_select:', data.index_select(1, torch.tensor([0,2])))\n",
    "print(':', data[:,1:2]) # 连续选择\n",
    "# 选择对应位置元素并返回一维张量\n",
    "print('masked_select:', data.masked_select(torch.tensor([[0,1,0],[1,0,1]], dtype=torch.uint8))) # 同维度对应，类似.reshape(-1)\n",
    "print('take:', data.take(torch.tensor([1,2,4]))) # .reshape(-1)后，按照下标检索\n",
    "# 拼接\n",
    "print('cat:', torch.cat([data, data], 0))\n",
    "print('stack:', torch.stack([data, data], 2).shape) # 新增一维度来拼接\n",
    "# 重复: 对应维度扩张倍数\n",
    "print('repeat:', data.repeat([2,2]).shape)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data= tensor([[0.8797, 0.1840, 0.0028],\n",
      "        [0.7228, 0.0723, 0.7652]])\n",
      "reciprocal: tensor([[  1.1367,   5.4338, 353.4427],\n",
      "        [  1.3836,  13.8256,   1.3069]])\n",
      "sqrt: tensor([[0.9379, 0.4290, 0.0532],\n",
      "        [0.8502, 0.2689, 0.8747]])\n",
      "rsqrt: tensor([[ 1.0662,  2.3311, 18.8001],\n",
      "        [ 1.1763,  3.7183,  1.1432]])\n",
      "pow tensor([[7.7388e-01, 3.3868e-02, 8.0050e-06],\n",
      "        [5.2238e-01, 5.2316e-03, 5.8551e-01]])\n",
      "exp: tensor([[2.4102, 1.2021, 1.0028],\n",
      "        [2.0601, 1.0750, 2.1494]])\n",
      "expm1: tensor([[1.4102, 0.2021, 0.0028],\n",
      "        [1.0601, 0.0750, 1.1494]])\n",
      "sigmoid: tensor([[0.7068, 0.5459, 0.5007],\n",
      "        [0.6732, 0.5181, 0.6825]])\n",
      "logit: tensor([[ 1.9896, -1.4893, -5.8649],\n",
      "        [ 0.9582, -2.5514,  1.1813]])\n",
      "log: tensor([[-0.1282, -1.6926, -5.8677],\n",
      "        [-0.3247, -2.6265, -0.2676]])\n",
      "log2: tensor([[-0.1849, -2.4420, -8.4653],\n",
      "        [-0.4684, -3.7893, -0.3861]])\n",
      "log10: tensor([[-0.0557, -0.7351, -2.5483],\n",
      "        [-0.1410, -1.1407, -0.1162]])\n",
      "log1p: tensor([[0.6311, 0.1689, 0.0028],\n",
      "        [0.5439, 0.0698, 0.5683]])\n",
      "sin: tensor([[0.7706, 0.1830, 0.0028],\n",
      "        [0.6615, 0.0723, 0.6927]])\n",
      "asin: tensor([[1.0752, 0.1851, 0.0028],\n",
      "        [0.8078, 0.0724, 0.8713]])\n",
      "cos: tensor([[0.6374, 0.9831, 1.0000],\n",
      "        [0.7500, 0.9974, 0.7213]])\n",
      "acos: tensor([[0.4956, 1.3857, 1.5680],\n",
      "        [0.7630, 1.4984, 0.6995]])\n",
      "tanh: tensor([[0.7063, 0.1820, 0.0028],\n",
      "        [0.6186, 0.0722, 0.6441]])\n",
      "atanh: tensor([[1.3745, 0.1862, 0.0028],\n",
      "        [0.9134, 0.0725, 1.0086]])\n",
      "sinh: tensor([[0.9976, 0.1851, 0.0028],\n",
      "        [0.7873, 0.0724, 0.8421]])\n",
      "cosh: tensor([[1.4125, 1.0170, 1.0000],\n",
      "        [1.2728, 1.0026, 1.3073]])\n",
      "sign: tensor([[-1., -1., -1.],\n",
      "        [-1., -1., -1.]])\n",
      "abs: tensor([[0.8797, 0.1840, 0.0028],\n",
      "        [0.7228, 0.0723, 0.7652]])\n",
      "floor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "ceil: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "round tensor([[1., 0., 0.],\n",
      "        [1., 0., 1.]])\n",
      "trunc: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "frac: tensor([[0.8797, 0.1840, 0.0028],\n",
      "        [0.7228, 0.0723, 0.7652]])\n",
      "fmod: tensor(1)\n",
      "remainder: tensor(1)\n",
      "clamp: tensor([[0.6000, 0.2000, 0.2000],\n",
      "        [0.6000, 0.2000, 0.6000]])\n",
      "dot: tensor(0.5555)\n",
      "mm: tensor([[0.8078, 0.6513],\n",
      "        [0.6513, 1.1131]])\n",
      "mv: tensor([0.6513, 1.1131])\n",
      "inverse: tensor([[ 2.3436, -1.3713],\n",
      "        [-1.3713,  1.7007]])\n",
      "einsum(ij,jk->jk): tensor([[0.8078, 0.6513],\n",
      "        [0.6513, 1.1131]])\n",
      "einsum(ij,jk,km->im): tensor([[1.1813, 0.1958, 0.5006],\n",
      "        [1.3775, 0.2004, 0.8536]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的计算\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "data = torch.rand((2,3))\n",
    "print('data=', data)\n",
    "\n",
    "# 基本运算符号：+, -, *, /, **\n",
    "\n",
    "# 倒数，开方，先开方后倒数\n",
    "print('reciprocal:', data.reciprocal())\n",
    "print('sqrt:', data.sqrt())\n",
    "print('rsqrt:', data.rsqrt())\n",
    "# 幂函数，指数, sigmoid，对数函数\n",
    "print('pow', data.pow(2)) # x^2\n",
    "print('exp:', data.exp()) # e^x\n",
    "print('expm1:', data.expm1()) # e^x - 1\n",
    "print('sigmoid:', data.sigmoid()) # 1/(1+e^x)\n",
    "print('logit:', data.logit()) # 1/(1+e^(-x))\n",
    "print('log:', data.log()) # 自然底数\n",
    "print('log2:', data.log2()) # 底数为2\n",
    "print('log10:', data.log10()) # 底数为10\n",
    "print('log1p:', data.log1p()) # ln(1+x)\n",
    "# 三角函数\n",
    "print('sin:', data.sin())\n",
    "print('asin:', data.asin())\n",
    "print('cos:', data.cos())\n",
    "print('acos:', data.acos())\n",
    "print('tanh:', data.tanh())\n",
    "print('atanh:', data.atanh())\n",
    "print('sinh:', data.sinh())\n",
    "print('cosh:', data.cosh())\n",
    "\n",
    "# 符号函数: 1为正数,-1为负\n",
    "print('sign:', (-data).sign())\n",
    "# 绝对值\n",
    "print('abs:', (-data).abs())\n",
    "# 下整数，上整数，四舍五入，取整数部分，取小数部分\n",
    "print('floor:', data.floor())\n",
    "print('ceil:', data.ceil())\n",
    "print('round', data.round())\n",
    "print('trunc:', data.trunc())\n",
    "print('frac:', data.frac())\n",
    "# 除法: 余数\n",
    "print('fmod:', torch.tensor(10).fmod(torch.tensor(3)))\n",
    "print('remainder:', torch.tensor(10).remainder(torch.tensor(3)))\n",
    "# 截断范围在某个范围内：限制范围\n",
    "print('clamp:', data.clamp(0.2,0.6))\n",
    "\n",
    "# 向量点积：仅限一维 同样大小，求和\n",
    "print('dot:', data[:,0].dot(data[:,2]))\n",
    "# 二维张量点积：\n",
    "print('mm:', data.mm(data.t()))\n",
    "# 二维张量和一维张量之间的点积: 最后一维相乘并求和\n",
    "print('mv:', data.mv(data[1,:]))\n",
    "\n",
    "# 矩阵求逆\n",
    "print('inverse:', data.mm(data.t()).inverse())\n",
    "\n",
    "# 复杂计算：求和约定，指定维度之间进行计算\n",
    "print('einsum(ij,jk->jk):', torch.einsum('ij,jk->ik', data, data.t()))\n",
    "print('einsum(ij,jk,km->im):', torch.einsum('ij,jk,km->im', data, data.t(), data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data= tensor([[0.2568, 0.9482, 0.4265],\n",
      "        [0.2140, 0.8346, 0.7309]])\n",
      "mean: tensor(0.5685)\n",
      "sum: tensor(3.4110)\n",
      "std: tensor(0.3112)\n",
      "var: tensor(0.0969)\n",
      "max: tensor(0.9482)\n",
      "min: tensor(0.2140)\n",
      "median: tensor(0.4265)\n",
      "norm: tensor(3.4110)\n",
      "kthvalue: torch.return_types.kthvalue(\n",
      "values=tensor([0.4265, 0.7309]),\n",
      "indices=tensor([2, 2]))\n",
      "prod: tensor(0.0136)\n",
      "==: tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "equal: True\n",
      "nonzero: tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2]])\n",
      "where: tensor([[0.2568, 0.9482, 0.4265],\n",
      "        [0.2140, 0.8346, 0.7309]])\n",
      "tensor(3.1415)\n"
     ]
    }
   ],
   "source": [
    "# 分布统计\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "data = torch.rand((2,3))\n",
    "print('data=', data)\n",
    "\n",
    "# 均值，求和，标准差, 方差，最大，最小，中位数，范数，第k大，积\n",
    "print('mean:', data.mean())\n",
    "print('sum:', data.sum())\n",
    "print('std:', data.std())\n",
    "print('var:', data.var())\n",
    "print('max:', data.max())\n",
    "print('min:', data.min())\n",
    "print('median:', data.median())\n",
    "print('norm:', data.norm(1)) # 几范数\n",
    "print('kthvalue:', data.kthvalue(2)) # 每行一个，返回位置\n",
    "print('prod:', data.prod())\n",
    "\n",
    "# 比较: >, <, >=, <=, ==, !=\n",
    "print('==:', data == data) # 逐个元素比较\n",
    "print('equal:', data.equal(data)) # 整体比较\n",
    "\n",
    "# nonzero\n",
    "print('nonzero:', data.nonzero()) # 返回位置元组的列表\n",
    "\n",
    "# where\n",
    "print('where:', data.where(data>torch.tensor(.2), torch.tensor(1.)))\n",
    "\n",
    "# 计算圆周率: pi*r^2 / 4 = p, 欧几里得空间上面积度量视为的均匀分布。\n",
    "samples = torch.rand(10000000,2) # r=1\n",
    "dist = samples.norm(p=2,dim=1)\n",
    "ratio = (dist<1).float().mean()\n",
    "print(ratio*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 变量\n",
    "import torch\n",
    "\n",
    "x = torch.autograd.Variable(torch.ones(2,2), requires_grad=True)\n",
    "print(x)\n",
    "y=x.mean()\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "print(x.grad.data)\n",
    "print(x.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集Dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 继承Dataset后，实现__len__(self)和__getitem__(self, idx)两个函数\n",
    "\n",
    "class NewDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "        return img, label\n",
    "\n",
    "newdataset = NewDataset()\n",
    "    \n",
    "# 数据集迭代读取器DataLoader，多线程读取，批量大小, shuffle\n",
    "dataloader = DataLoader(newdataset, batch_size=32, shuffle=True, num_workers=3)\n",
    "\n",
    "for imags, labels in dataloader:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "# 动态修改学习速率 lr_scheduler.StepLR\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.1)\n",
    "from torch.optim import lr_sheduler\n",
    "# 按照训练步数改变：每step_size步为原来gamma\n",
    "lr_sheduler = lr_sheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# 按照指定步数改变：乘以gamma\n",
    "multi_lr_sheduler = lr_sheduler.MultiStepLR(optimizer, milestones=[10,100], gamma=0.1)\n",
    "# 每步乘以gamma\n",
    "exp_lr_sheduler = lr_sheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "# 按照特定指标变化修改\n",
    "reduce_lr_sheduler = lr_sheduler.ReduceLROnPlateau(optmizer, 'min')\n",
    "reduce_lr_sheduler(validate_loss)\n",
    "\n",
    "\n",
    "# 模型保存与加载\n",
    "msd = model.state_dict()\n",
    "model.train(True)\n",
    "model.train(False)  \n",
    "model.load_state_dict(msd)\n",
    "\n",
    "\n",
    "# 过拟合根本原因是：数据量 和 模型容量 之间不匹配\n",
    "# 策略1: 数据量增加，行数、特征或知识，方式包括多任务正则学习和预训练等\n",
    "# 策略2: 模型容量降低，包括正则项、dropout、设计容量低模型、学习率\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
